2024-11-22 15:17:22:INFO:Start running self_mm...
2024-11-22 15:17:22:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'self_mm', 'datasetName': 'sims', 'num_workers': 0, 'model_save_dir': 'results_copy/models', 'res_save_dir': 'results_copy/results\\normals', 'gpu_ids': [0], 'annealing_step': 20, 'gamma': 1, 'epochs': 100, 'seeds': [1111], 'dataPath': 'E:\\yanyi\\code of paper\\Self-MM-main\\Self-MM-main\\Dataset\\SIMS/unaligned_39.pkl', 'seq_lens': (39, 400, 55), 'feature_dims': (768, 33, 709), 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'need_normalized': False, 'use_bert': True, 'use_finetune': True, 'save_labels': False, 'early_stop': 8, 'update_epochs': 4, 'batch_size': 32, 'learning_rate_bert': 5e-05, 'learning_rate_audio': 0.001, 'learning_rate_video': 0.0001, 'learning_rate_other': 0.001, 'weight_decay_bert': 0.001, 'weight_decay_audio': 0.001, 'weight_decay_video': 0.001, 'weight_decay_other': 0.001, 'a_lstm_hidden_size': 16, 'v_lstm_hidden_size': 64, 'a_lstm_layers': 1, 'v_lstm_layers': 1, 'text_out': 768, 'audio_out': 16, 'video_out': 32, 'a_lstm_dropout': 0.2, 'v_lstm_dropout': 0.2, 't_bert_dropout': 0.2, 'post_fusion_dim': 128, 'post_text_dim': 64, 'post_audio_dim': 16, 'post_video_dim': 32, 'post_fusion_dropout': 0.0, 'post_text_dropout': 0.2, 'post_audio_dropout': 0.2, 'post_video_dropout': 0.2, 'H': 1.0, 'seed': 1111}>
2024-11-22 15:17:22:INFO:Let's use 1 GPUs!
2024-11-22 15:17:29:INFO:train samples: (1368,)
2024-11-22 15:17:30:INFO:valid samples: (456,)
2024-11-22 15:17:30:INFO:test samples: (457,)
2024-11-22 15:17:30:DEBUG:Starting new HTTPS connection (1): huggingface.co:443
2024-11-22 15:17:38:DEBUG:https://huggingface.co:443 "HEAD /bert-base-chinese/resolve/main/tokenizer_config.json HTTP/11" 200 0
2024-11-22 15:17:39:DEBUG:https://huggingface.co:443 "HEAD /bert-base-chinese/resolve/main/config.json HTTP/11" 200 0
2024-11-22 15:17:39:DEBUG:https://huggingface.co:443 "HEAD /bert-base-chinese/resolve/main/config.json HTTP/11" 200 0
2024-11-22 15:17:53:INFO:The model has 102474139 trainable parameters
2024-11-22 15:19:26:INFO:Start running self_mm...
2024-11-22 15:19:26:INFO:<Storage{'is_tune': False, 'train_mode': 'classification', 'modelName': 'self_mm', 'datasetName': 'sims', 'num_workers': 0, 'model_save_dir': 'results_copy/models', 'res_save_dir': 'results_copy/results\\normals', 'gpu_ids': [0], 'annealing_step': 20, 'gamma': 1, 'epochs': 100, 'seeds': [1111], 'dataPath': 'E:\\yanyi\\code of paper\\MISA-master\\MISA-master\\Dataset\\SIMS/unaligned_39.pkl', 'seq_lens': (39, 400, 55), 'feature_dims': (768, 33, 709), 'train_samples': 1368, 'num_classes': 3, 'language': 'cn', 'KeyEval': 'Loss', 'need_data_aligned': False, 'need_model_aligned': False, 'need_normalized': False, 'use_bert': True, 'use_finetune': True, 'save_labels': False, 'early_stop': 8, 'update_epochs': 4, 'batch_size': 32, 'learning_rate_bert': 5e-05, 'learning_rate_audio': 0.001, 'learning_rate_video': 0.0001, 'learning_rate_other': 0.001, 'weight_decay_bert': 0.001, 'weight_decay_audio': 0.001, 'weight_decay_video': 0.001, 'weight_decay_other': 0.001, 'a_lstm_hidden_size': 16, 'v_lstm_hidden_size': 64, 'a_lstm_layers': 1, 'v_lstm_layers': 1, 'text_out': 768, 'audio_out': 16, 'video_out': 32, 'a_lstm_dropout': 0.2, 'v_lstm_dropout': 0.2, 't_bert_dropout': 0.2, 'post_fusion_dim': 128, 'post_text_dim': 64, 'post_audio_dim': 16, 'post_video_dim': 32, 'post_fusion_dropout': 0.0, 'post_text_dropout': 0.2, 'post_audio_dropout': 0.2, 'post_video_dropout': 0.2, 'H': 1.0, 'seed': 1111}>
2024-11-22 15:19:26:INFO:Let's use 1 GPUs!
2024-11-22 15:19:33:INFO:train samples: (1368,)
2024-11-22 15:19:34:INFO:valid samples: (456,)
2024-11-22 15:19:34:INFO:test samples: (457,)
2024-11-22 15:19:34:DEBUG:Starting new HTTPS connection (1): huggingface.co:443
